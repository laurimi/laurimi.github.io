<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://laurimi.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://laurimi.github.io/" rel="alternate" type="text/html" /><updated>2020-10-23T09:32:14+02:00</updated><id>https://laurimi.github.io/feed.xml</id><title type="html">Mikko Lauri</title><subtitle>Researcher in Computer Science and Robotics. Multi-agent  decision-making under uncertainty, active perception,  and deep learning for robot vision.</subtitle><author><name>Mikko Lauri</name></author><entry><title type="html">New results on multi-agent active perception at NeurIPS 2020</title><link href="https://laurimi.github.io/research/2020/10/23/neurips.html" rel="alternate" type="text/html" title="New results on multi-agent active perception at NeurIPS 2020" /><published>2020-10-23T00:00:00+02:00</published><updated>2020-10-23T00:00:00+02:00</updated><id>https://laurimi.github.io/research/2020/10/23/neurips</id><content type="html" xml:base="https://laurimi.github.io/research/2020/10/23/neurips.html">&lt;p&gt;We have a new paper entitled “Multi-agent active perception with prediction rewards” up on &lt;a href=&quot;https://arxiv.org/abs/2010.11835&quot;&gt;arXiv&lt;/a&gt; due to appear later at the &lt;a href=&quot;https://neurips.cc/Conferences/2020&quot;&gt;34th Conference on Neural Information Processing Systems (NeurIPS 2020)&lt;/a&gt;. 
This is research done together with &lt;a href=&quot;http://www.fransoliehoek.net/&quot; target=&quot;_blank&quot;&gt;Frans A. Oliehoek&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;We propose a reduction from a Dec-POMDP with a convex information reward to a standard Dec-POMDP. Now all standard Dec-POMDP algorithms are applicable to multi-agent active perception problems, and we can solve larger scale problems.&lt;/p&gt;

&lt;h1 id=&quot;in-more-detail&quot;&gt;In more detail&lt;/h1&gt;
&lt;p&gt;The setting of the paper is illustrated in the figure below.
The multi-agent active perception problem consists of two phases: the decentralized observation gathering phase (left) where agents act independently based on their private information, and the centralized inference phase (right) after the end of the task, where a belief state is computed and the agents are rewarded according to the informativeness of the belief state.
A solution is a set of individual policies, one for each of the agents, that an agent can execute in a decentralized manner.
At the end of the task after a finite number of time steps, the observations collected by the agents are collected at a central server for inference.
This enables computation of a state estimate, or belief state, and the overall reward of the agent team is determined by the informativeness of the state estimate, measured for instance by the negative entropy.
This type of problem formulation could be applied to settings like search and rescue robotics, multi-robot environment monitoring missions, or distributed sensor networks.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/assets/images/neurips2020/thumbnail.png&quot;&gt;&lt;img src=&quot;/assets/images/neurips2020/thumbnail.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;The multi-agent active perception problem.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This problem can be modelled as a decentralized partially observable Markov decision process (Dec-POMDP) with a special kind of information reward, as I did &lt;a href=&quot;/research/2020/06/13/active-perception.html&quot;&gt;in my earlier work&lt;/a&gt;.
Because the information reward depends on what the agents &lt;em&gt;know&lt;/em&gt; about the state rather than what the state &lt;em&gt;is&lt;/em&gt;, most standard algorithms for Dec-POMDPs are not applicable to this problem, and indeed we proposed some specialized algorithms in earlier work to tackle this.
However, one of the issues of these algorithms is that to search for policies, we have to compute and store in memory all of the possible state estimates that could be reached during the task.
This is very memory intensive, and as a result these algorithms have trouble scaling up.&lt;/p&gt;

&lt;p&gt;In the new paper, we address these two issues in a neat way: we propose a conversion of the entire problem into a completely standard form Dec-POMDP, with the typical kind of reward function!
This means that all of the standard algorithms for Dec-POMDPs are now applicable to solve multi-agent active perception problems.
Modern Dec-POMDP solvers do not need to compute all reachable belief states, but can instead rely on techniques such as Monte Carlo sampling to evaluate and search for policies.
Technically, we achieve the conversion by introducing so-called &lt;em&gt;prediction actions&lt;/em&gt;, inspired by similar earlier work in centralize control settings.
To put things briefly and informally, the prediction actions “decentralize” the state estimation problem in a way that each agent will also make a prediction about the true state at the end of the observation gathering phase.&lt;/p&gt;

&lt;p&gt;The conversion we propose induces a loss, but we show that the loss is bounded and that there are cases where the loss is zero.
We show experimentally that by our conversion, scalability in the planning horizon is greatly improved compared to previous methods.&lt;/p&gt;

&lt;p&gt;I’m also excited about the opportunities for future research the results open up.
By linking multi-agent active perception to standard Dec-POMDPs, any advances in one are immediately transferable to the other.
Standard Dec-POMDPs are also the most popular underlying formalism for multi-agent reinforcement learning under partial observability, so by this reduction we potentially enable &lt;em&gt;learning for multi-agent active perception&lt;/em&gt;.
Looking deeper at when the loss due to the conversion is small could further boost the practical applicability of the method.&lt;/p&gt;

&lt;p&gt;You can read about the details &lt;a href=&quot;https://arxiv.org/abs/2010.11835&quot;&gt;in the paper&lt;/a&gt;, and try out the conversion and a solver algorithm by checking out &lt;a href=&quot;https://github.com/laurimi/multiagent-prediction-reward&quot;&gt;the code&lt;/a&gt;.&lt;/p&gt;</content><author><name>Mikko Lauri</name></author><summary type="html">We have a new paper entitled “Multi-agent active perception with prediction rewards” up on arXiv due to appear later at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020). This is research done together with Frans A. Oliehoek.</summary></entry><entry><title type="html">Multi-robot next-best-view planning</title><link href="https://laurimi.github.io/research/2020/07/04/multirobot-nbv.html" rel="alternate" type="text/html" title="Multi-robot next-best-view planning" /><published>2020-07-04T00:00:00+02:00</published><updated>2020-07-04T00:00:00+02:00</updated><id>https://laurimi.github.io/research/2020/07/04/multirobot-nbv</id><content type="html" xml:base="https://laurimi.github.io/research/2020/07/04/multirobot-nbv.html">&lt;p&gt;&lt;em&gt;In this post, I give an overview of my recent paper &lt;a class=&quot;citation&quot; href=&quot;#Lauri_RAL2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt; on multi-robot next-best-view planning.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Robots need three-dimensional scene models for many tasks ranging from manipulation to mapping and motion planning.
To build a scene model, the robot needs to collect data, for example depth images of point clouds, from many different viewpoints.
These data are then fused together to create the scene model in a process known as scene reconstruction.&lt;/p&gt;

&lt;p&gt;Next-best-view (NBV) planning refers to intelligently choosing &lt;em&gt;how&lt;/em&gt; and &lt;em&gt;where&lt;/em&gt; the robot should capture the next images.
The objective is to reduce the time and resources required to obtain a scene reconstruction of sufficient quality.
Single-robot NBV planning therefore often considers utility functions such as the expected information gain available from a particular view.&lt;/p&gt;

&lt;p&gt;When multiple robots are used for scene reconstruction, the robots should coordinate their actions.
They should avoid views that overlap strongly, because that is inefficient.
NBV planning for multiple robots therefore needs to consider not only the information gain for a single robot, but for the team of robots as a whole, as illustrated in the figure below.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/waste_sm.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/waste_sm.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/view_example.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/view_example.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Left: Two robots reconstructing a tabletop scene. Right: Point clouds captured by the robots, and the fields of view of the robots' cameras (red and blue frustums).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Let us assume we have a team of \(n\) robots performing scene reconstruction.
Each robot \( i = 1, 2, \ldots, n \) may choose its next view \(x_i\) from some set \(X_i \) of potential views.
The utility of selecting this set of views is measured by a &lt;em&gt;utility function&lt;/em&gt; \(f \) that maps tuples of views to a real number proportional to “how good” these views are.
The NBV planning problem can be stated as a maximization problem
\begin{equation}
\max\limits_{\forall i: x_i \in X_i} f(x_1, x_2, \ldots, x_n),
\end{equation}
that is, maximize the utility over all possible valid choices of views for each robot.&lt;/p&gt;

&lt;p&gt;In our recent paper &lt;a class=&quot;citation&quot; href=&quot;#Lauri_RAL2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt; we make two contributions to solving these kinds of multi-robot NBV planning problems:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We show that a utility function \(f\) can be designed which encourages the robots to avoid selecting overlapping views. Furthermore, this utility function turns out to be a &lt;em&gt;monotonically increasing and submodular&lt;/em&gt; function.&lt;/li&gt;
  &lt;li&gt;A simple polynomial-time, greedy algorithm can solve the maximization problem above with bounded error, that is, the solution found by the greedy algorithm is withing a constant factor from an optimal solution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, I describe the technical background of our results, the implementation of the optimization algorithm we used, along with some thoughts about future work that still remains to be done.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Before jumping into solving multi-robot NBV planning problems, we look at two mathematical concepts that enable our results: matroids and submodularity.&lt;/p&gt;

&lt;h3 id=&quot;matroids&quot;&gt;Matroids&lt;/h3&gt;
&lt;p&gt;Matroids generalize the notion of independence from vectors to sets.
A matroid \( (\Omega, \mathcal{I})\) consists of a &lt;em&gt;ground set&lt;/em&gt; \( \Omega\) and a family of independent sets \( \mathcal{I}\).
Every element \( I \in \mathcal{I} \) is an &lt;em&gt;independent set&lt;/em&gt;, and is a subset of the ground set.
Matroid also need to satisfy a few other properties, a more complete discussion may be found in &lt;a class=&quot;citation&quot; href=&quot;#Oxley2003&quot;&gt;(Oxley, 2003)&lt;/a&gt; or a textbook on the subject.&lt;/p&gt;

&lt;p&gt;In our case, the ground set is the union of all robots’ views, that is, \( \Omega = \bigcup\limits_{i=1}^n X_i\).
The purpose of the matroid is to represent all valid views selectable by the team of robots by &lt;em&gt;constraining&lt;/em&gt; the ground set.
In particular, only subsets of \( \Omega\) that contain &lt;em&gt;at most&lt;/em&gt; one view per robot are valid.
Formally, this leads to the family of independent sets
\begin{equation}
\mathcal{I} = \left\lbrace I \subseteq \Omega \middle| \forall i: |X_i \cap I| \leq 1 \right\rbrace.
\end{equation}
This is also known as a &lt;em&gt;block matroid&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The block matroid \( (\Omega, \mathcal{I})\) therefore depicts all valid view combinationss, where each robot chooses either no view at all or a single view.
The maximization problem from above can be stated as a matroid-constrained maximization problem
\begin{equation}
\max\limits_{I \in \mathcal{I}} f(I).
\end{equation}
Next, we look at a property of the utility function \(f \) that makes this problem easy to approximate.&lt;/p&gt;

&lt;h3 id=&quot;submodularity&quot;&gt;Submodularity&lt;/h3&gt;
&lt;p&gt;The utility function \(f\) above maps a tuple of views to a real number.
Equivalently, we can think of it as a set function \(f: 2^{\Omega} \to \mathbb{R} \) that maps subsets of the ground set \( \Omega\) to a real number.
Here, \(2^\Omega\) denotes the power set of \( \Omega\) – the set of all subsets of \(\Omega\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Submodularity&lt;/em&gt; is an interesting property of set functions.
It can be thought of as a diminishing returns property: adding a new element \(x \in \Omega \) to a subset \(A \subset \Omega\) improves the value of \(f\) more the smaller the subset is.
Submodularity appears in many applications, for example in sensor placement &lt;a class=&quot;citation&quot; href=&quot;#krause2008near&quot;&gt;(Krause et al., 2008)&lt;/a&gt;.
Suppose you are considering deploying a new sensor.
Intuitively, if you already have deployd tons of other sensors, the marginal utility of adding this new sensor is not as great compared to if you only have a few sensors so far.
Submodularity formalizes this intuition.&lt;/p&gt;

&lt;p&gt;Given a set function \(f\), it is submodular if for any \( A \subset B \subset \Omega\) and any \( x \in \Omega \setminus B \), \(f(A \cup \{x\}) - f(A) \geq f(B \cup \{x\}) - f(B)\).
Further, \(f\) is said to be monotonically increasing, if for any \(A \subset \Omega \) and \( x \in \Omega \setminus A\), \(f(A \cup \{x\}) \geq f(A)\).&lt;/p&gt;

&lt;h3 id=&quot;maximizing-a-monotonically-increasing-submodular-function&quot;&gt;Maximizing a monotonically increasing submodular function&lt;/h3&gt;
&lt;p&gt;Let’s consider a greedy algorithm for solving the maximization problem \(\max\limits_{I \in \mathcal{I}} f(I)\).
Start with the solution \(I_0 = \emptyset\).
Then, at steps \(k = 1, \ldots, n \),&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Find the element \(x^* \in \Omega\) that maximizes the marginal utility \(f(I_{k-1} \cup \{x^*\}) - f(I_{k-1})\) such that \( I_{k-1} \cup \{x^*\}\) still satisfies the matroid constraint, that is, \(I_{k-1} \cup \{x^*\} \in \mathcal{I} \)&lt;/li&gt;
  &lt;li&gt;Assign \(I_k = I_{k-1} \cup \{x^*\}\) and if \(k &amp;lt; n\), incremnent \(k\) and go back to 1, otherwise return \(I_k\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If \(f\) is monotonically increasing and submodular, then the solution \(I_n\) returned by this simple greedy algorithm is a bounded approximation.
Specifically, it is within a factor of \( \frac{1}{2}\) from the optimal solution: \(f(I_n) \geq \frac{1}{2} \max\limits_{I \in \mathcal{I}} f(I) \).
A proof of this result is in &lt;a class=&quot;citation&quot; href=&quot;#fisher1978analysis&quot;&gt;(Fisher et al., 1978)&lt;/a&gt;.
This is interesting for many applications where submodularity appears, because a polynomial-time algorithm can be guaranteed to provide a bounded approximation to the overall optimization problem.&lt;/p&gt;

&lt;h2 id=&quot;multi-robot-next-best-view-planning&quot;&gt;Multi-robot next-best-view planning&lt;/h2&gt;
&lt;p&gt;What remains to be done is to come up with a utility function \(f \) that is suitable for multi-robot NBV planning, and is submodular and monotonically increasing.
Then, we can use greedy maximization to solve multi-robot NBV planning problems.&lt;/p&gt;

&lt;h3 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;We make two assumptions about the problem at hand.
Firstly, we assume the robots can communicate to coordinate their next sensing actions.
This is reasonable in settings where the robots are deployed in close proximity with reliable communication, e.g., industrial robots on the same wired network in a warehouse.
The problem we consider is therefore a centralized optimization problem.
However, the same kinds of methods can also be applied to distributed problems where the robots do not constantly communicate, see for example &lt;a class=&quot;citation&quot; href=&quot;#corah2019distributed&quot;&gt;(Corah &amp;amp; Michael, 2019)&lt;/a&gt;.
Secondly, we assume the potential views of each robot have already been determined, that is, the sets \(X_i\) are known and fixed.&lt;/p&gt;

&lt;h3 id=&quot;utility-function&quot;&gt;Utility function&lt;/h3&gt;
&lt;p&gt;In the paper we propose an overlap-aware utility function for multirobot NBV planning.
This utility function is based on applying raytracing in the current scene reconstruction to find out which views are likely to result in a high information gain.
Overlap is avoided by not rewarding the robots multiple times for observing the same part of the scene.&lt;/p&gt;

&lt;p&gt;We use an &lt;a href=&quot;https://octomap.github.io/&quot;&gt;OctoMap&lt;/a&gt; to model the environment as a three-dimensional voxel grid.
The OctoMap package already provides tools for raytracing, which makes working with it very convenient in this case.
Let’s say there are three robots, and we wish to evalute the usefulness of them selecting the views \(x_1, x_2, x_3 \), respectively.
This situation is conceptually illustrated in the figure below.
The view \(x_i\) of each robot consists of rays \(r_{i,j}\) that pass through some voxels.
These rays indicate the part of the environment potentially seen by each robot, and is illustrated by the shaded voxels.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/overlap.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/overlap.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Rays (solid lines) emitted from three different views, and the corresponding visible voxels (shaded).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the example above, views \(x_1\) and \(x_2\) both observe the same voxel \(v\) indicated in the figure – this is not very useful!
Even one robot seeing the voxel is sufficient, while the other could use its camera to look elsewhere.
Taking this into account is the key feature of our overlap-aware utility function.
Informally, our utility function satisfies the following:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;For each voxel \(v\) in the environment, the robots gain utility only once, and observing the same voxel from multiple views is not useful.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the example above, out of all views that could possibly observe \(v\), only the best one is considered by our utility function.&lt;/p&gt;

&lt;p&gt;In the paper &lt;a class=&quot;citation&quot; href=&quot;#Lauri_RAL2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt;, we prove that our overlap-aware utility function is a monotonically increasing and submodular function.
As we saw earlier, maximization problems involving submodular functions are approximately solvable by a greedy algorithm.
What is also interesting is that the our utility function generalizes several other utility functions proposed for the single-robot case &lt;a class=&quot;citation&quot; href=&quot;#delmerico2018comparison&quot;&gt;(Delmerico et al., 2018)&lt;/a&gt;, helping justify why the greedy optimization strategy is also appropriate in single-robot settings.&lt;/p&gt;

&lt;h3 id=&quot;greedy-maximization-for-next-best-view-planning&quot;&gt;Greedy maximization for next-best-view planning&lt;/h3&gt;
&lt;p&gt;Our proposed utility function can be separated into a sum of per-voxel utilities \(g_v(I)\), where \( v \) is a voxel and \(I\) is an independent set of views.
This separation property also makes it easy to implement the greedy maximization algorithm for NBV planning, similar to the one outlined earlier.
The algorithm has an initialization stage, which is the only step where costly raytracing is required.
After the initialization stage, the maximization can be done simply by querying data from a lookup table.&lt;/p&gt;

&lt;p&gt;In the initialization stage, we process for each robot \(i\) each of its candidate views \(x_i \in X_i\).
By raytracing, we can determine the subset of voxels that is potentially visible at view \(x_i\).
For all such voxels \(v\), we save the per-voxel utility \(g_v(x_i)\) into a data structure.&lt;/p&gt;

&lt;p&gt;When we want to find an independent set \( I \) that maximizes the utility \(f\), we proceed similar to the simple greedy algorithm earlier.
First, we choose the view \(x\) that has the greatest sum of per-voxel utilities \( g_v(x)\) over all potentially visible voxels.
Notice that there is no subscript \(i\) as the view \(x\) could be for any of the robots.
Our initial independent set is \( I_1 = \{ x \}\).
The per-voxel utility of the independent set \(I_1 \) is initialized to \(g_v(I_1) = g_v(x) \).&lt;/p&gt;

&lt;p&gt;At any subsequent step \(k \), we have an independent set \(I_k\).
We then process the views of all robots \( i\) whose views are not yet in \(I_k\).
We calculate the marginal utility of view \( x_i\) as follows.
First we initalize the marginal utility \(M(x_i)\) to zero.
We loop over all visible voxels \(v\) at \(x_i\).
For each such voxel, there are two possibilities.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The voxel was not seen by any view so far. Then we add \(g_v(x_i)\) to \(M(x_i)\).&lt;/li&gt;
  &lt;li&gt;The voxel was seen by at least one view in \(I_k\). If \(g_v(x_i) &amp;gt; g_v(I_k) \), the new view is better for observing \(v\), and we add \(g_v(x_i) - g_v(I_k)\) to \(M(x_i)\). Otherwise, the old views are better and we do not add anything to \(M(x_i)\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, we select the view \(x\) that has the greatest marginal utility \(M(x)\) and add it to \(I_k \) to obtain \(I_{k+1}\).
The process is repeated until \(k = n\), and we have a view planned for each robot.&lt;/p&gt;

&lt;h3 id=&quot;how-well-does-it-work&quot;&gt;How well does it work?&lt;/h3&gt;
&lt;p&gt;We evaluated the greedy algorithm for multi-robot NBV in simulated and real-world scene reconstruction tasks with multiple robots.
The figure below shows an example of two simulated environments we experimented with.
In addition to what is shown in the figures, randomly sampled furniture and other objects were present in the simulations.
We used scenes from &lt;a href=&quot;https://robotvault.bitbucket.io/scenenet-rgbd.html&quot;&gt;SceneNet RGBD&lt;/a&gt; and applied &lt;a href=&quot;https://octomap.github.io/&quot;&gt;OctoMap&lt;/a&gt; for the map representation and raytracing tasks.
The single-robot &lt;a href=&quot;https://github.com/ethz-asl/volumetric_mapping&quot;&gt;volumetric mapping ROS package&lt;/a&gt; also provided a lot of inspiration for the implementation.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/bathroom8.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/bathroom8.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/kitchen11.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/kitchen11.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Left: The &quot;Bathroom 8&quot; environment. Right: The &quot;Kitchen 11&quot; environment.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We compared our multi-robot NBV algorithm to randomly selecting views and to planning for each robot separately (ignoring coordination of view planning, leading to more overlap).
We measured the &lt;em&gt;surface coverage percentage&lt;/em&gt; as a function of the number of views taken by each of 8 robots in the environment.
The results are summarized in the plot below, showing that our method works better than the baselines.
Bathroom 8 consists of many rooms with occluders, so planning is more important and effective here.
This is reflected in the good performance of our method compared to random planning.
Kitchen 11 is a single room with no occluding walls, and here the baseline methods also work well as the reconstruction task overall is easier.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/sc_bathroom8.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/sc_bathroom8.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/assets/images/multirobot-nbv/sc_kitchen11.png&quot;&gt;&lt;img src=&quot;/assets/images/multirobot-nbv/sc_kitchen11.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Surface coverage as a function of number of views per robot. Number in parentheses in the legend indicates area under curve (AUC).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also deployed our method to a setting with two real robots, as shown in the images at the top of this post.
Planning with our utility function that takes overlap into account also worked well in this setting.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and future work&lt;/h2&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#Lauri_RAL2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt; we proposed an overlap-aware utility function for next-best-view (NBV) planning that is applicable to multi-robot settings.
There are several interesting directions of future work that we did not explore yet.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We used a fixed set of candidate viewpoints. The trade-off between having more viewpoints and increased computational time is not clear yet. It might also be useful to create the candidate viewpoints dynamically, instead of working with fixed sets \(X_i\) of views for each robot.&lt;/li&gt;
  &lt;li&gt;As mentioned earlier, many utility functions have been proposed for single-robot NBV planning &lt;a class=&quot;citation&quot; href=&quot;#delmerico2018comparison&quot;&gt;(Delmerico et al., 2018)&lt;/a&gt;. Our overlap-aware utility function generalizes several of these, but we did not study in detail which utility functions might be most useful for which multi-robot application.&lt;/li&gt;
  &lt;li&gt;We also did not tune the parameters of our algorithm in detail, so it remains to be investigated how for example the number of candidate views, raytracing resolution, voxel map resolution, etc., should be set for various applications.&lt;/li&gt;
  &lt;li&gt;Constant communication is assumed in the centralized setting. However, matroid-constrained submodular maximization is also applicable with less restrictive centralization assumptions &lt;a class=&quot;citation&quot; href=&quot;#corah2019distributed&quot;&gt;(Corah &amp;amp; Michael, 2019)&lt;/a&gt;, potentially applicable to NBV planning as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am hoping future research will shed a light on these issues as well!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Lauri_RAL2020&quot;&gt;&lt;b&gt;Lauri, M.&lt;/b&gt;, Pajarinen, J., Peters, J., &amp;amp; Frintrop, S. (2020). Multi-Sensor Next-Best-View Planning as Matroid-Constrained Submodular Maximization. &lt;i&gt;IEEE Robotics and Automation Letters&lt;/i&gt;, &lt;i&gt;5&lt;/i&gt;(4), 5323–5330. https://doi.org/10.1109/LRA.2020.3007445&lt;/span&gt;&lt;/div&gt;



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1109/LRA.2020.3007445&quot;&gt;DOI&lt;/a&gt;]


	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://arxiv.org/abs/2007.02084&quot;&gt;arXiv&lt;/a&gt;]




[&lt;a onclick=&quot;toggleBibtexLauri_RAL2020()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexLauri_RAL2020()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aLauri_RAL2020&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bLauri_RAL2020&quot;&gt;@article{Lauri_RAL2020,
  author = {Lauri, Mikko and Pajarinen, Joni and Peters, Jan and Frintrop, Simone},
  title = {Multi-Sensor Next-Best-View Planning as Matroid-Constrained Submodular Maximization},
  year = {2020},
  doi = {10.1109/LRA.2020.3007445},
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  month = oct,
  pages = {5323--5330},
  archiveprefix = {arXiv},
  eprint = {2007.02084}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexLauri_RAL2020(parameter) {
    var x= document.getElementById('aLauri_RAL2020');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexLauri_RAL2020(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bLauri_RAL2020').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Oxley2003&quot;&gt;Oxley, J. (2003). What is a Matroid? &lt;i&gt;CUBO, A Mathematical Journal&lt;/i&gt;, &lt;i&gt;5&lt;/i&gt;(3), 176–215.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexOxley2003()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexOxley2003()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aOxley2003&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bOxley2003&quot;&gt;@article{Oxley2003,
  title = {What is a Matroid?},
  volume = {5},
  number = {3},
  journal = {CUBO, A Mathematical Journal},
  author = {Oxley, James},
  year = {2003},
  pages = {176--215}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexOxley2003(parameter) {
    var x= document.getElementById('aOxley2003');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexOxley2003(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bOxley2003').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;krause2008near&quot;&gt;Krause, A., Singh, A., &amp;amp; Guestrin, C. (2008). Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(Feb), 235–284.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexkrause2008near()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexkrause2008near()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;akrause2008near&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bkrause2008near&quot;&gt;@article{krause2008near,
  title = {Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies},
  author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {Feb},
  pages = {235--284},
  year = {2008}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexkrause2008near(parameter) {
    var x= document.getElementById('akrause2008near');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexkrause2008near(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bkrause2008near').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;fisher1978analysis&quot;&gt;Fisher, M. L., Nemhauser, G. L., &amp;amp; Wolsey, L. A. (1978). An analysis of approximations for maximizing submodular set functions – II. In &lt;i&gt;Polyhedral combinatorics&lt;/i&gt; (pp. 73–87). Springer.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexfisher1978analysis()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexfisher1978analysis()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;afisher1978analysis&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bfisher1978analysis&quot;&gt;@incollection{fisher1978analysis,
  title = {An analysis of approximations for maximizing submodular set functions -- {{II}}},
  author = {Fisher, Marshall L and Nemhauser, George L and Wolsey, Laurence A},
  booktitle = {Polyhedral combinatorics},
  pages = {73--87},
  year = {1978},
  publisher = {Springer}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexfisher1978analysis(parameter) {
    var x= document.getElementById('afisher1978analysis');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexfisher1978analysis(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bfisher1978analysis').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;corah2019distributed&quot;&gt;Corah, M., &amp;amp; Michael, N. (2019). Distributed matroid-constrained submodular maximization for multi-robot exploration: Theory and practice. &lt;i&gt;Autonomous Robots&lt;/i&gt;, &lt;i&gt;43&lt;/i&gt;(2), 485–501. https://doi.org/10.1007/s10514-018-9778-6&lt;/span&gt;&lt;/div&gt;



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1007/s10514-018-9778-6&quot;&gt;DOI&lt;/a&gt;]





[&lt;a onclick=&quot;toggleBibtexcorah2019distributed()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexcorah2019distributed()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;acorah2019distributed&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bcorah2019distributed&quot;&gt;@article{corah2019distributed,
  title = {Distributed matroid-constrained submodular maximization for multi-robot exploration: Theory and practice},
  author = {Corah, Micah and Michael, Nathan},
  journal = {Autonomous Robots},
  volume = {43},
  number = {2},
  pages = {485--501},
  year = {2019},
  doi = {10.1007/s10514-018-9778-6},
  publisher = {Springer}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexcorah2019distributed(parameter) {
    var x= document.getElementById('acorah2019distributed');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexcorah2019distributed(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bcorah2019distributed').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;delmerico2018comparison&quot;&gt;Delmerico, J., Isler, S., Sabzevari, R., &amp;amp; Scaramuzza, D. (2018). A comparison of volumetric information gain metrics for active 3D object reconstruction. &lt;i&gt;Autonomous Robots&lt;/i&gt;, &lt;i&gt;42&lt;/i&gt;(2), 197–208. https://doi.org/10.1007/s10514-017-9634-0&lt;/span&gt;&lt;/div&gt;



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1007/s10514-017-9634-0&quot;&gt;DOI&lt;/a&gt;]





[&lt;a onclick=&quot;toggleBibtexdelmerico2018comparison()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexdelmerico2018comparison()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;adelmerico2018comparison&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bdelmerico2018comparison&quot;&gt;@article{delmerico2018comparison,
  title = {A comparison of volumetric information gain metrics for active 3D object reconstruction},
  author = {Delmerico, Jeffrey and Isler, Stefan and Sabzevari, Reza and Scaramuzza, Davide},
  journal = {Autonomous Robots},
  volume = {42},
  number = {2},
  pages = {197--208},
  year = {2018},
  doi = {10.1007/s10514-017-9634-0},
  publisher = {Springer}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexdelmerico2018comparison(parameter) {
    var x= document.getElementById('adelmerico2018comparison');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexdelmerico2018comparison(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bdelmerico2018comparison').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Mikko Lauri</name></author><summary type="html">In this post, I give an overview of my recent paper (Lauri et al., 2020) on multi-robot next-best-view planning.</summary></entry><entry><title type="html">Active perception and decentralized decision-making</title><link href="https://laurimi.github.io/research/2020/06/13/active-perception.html" rel="alternate" type="text/html" title="Active perception and decentralized decision-making" /><published>2020-06-13T00:00:00+02:00</published><updated>2020-06-13T00:00:00+02:00</updated><id>https://laurimi.github.io/research/2020/06/13/active-perception</id><content type="html" xml:base="https://laurimi.github.io/research/2020/06/13/active-perception.html">&lt;p&gt;&lt;em&gt;In this post, I give an overview of my recent research on multi-agent active perception, published in the two recent papers &lt;a class=&quot;citation&quot; href=&quot;#Lauri_JAAMAS2020&quot;&gt;(Lauri et al., 2020; Lauri et al., 2019)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Active perception is a process by which an agent, a software agent or an embodied agent such as a robot, perceives its environment to reduce uncertainty in order to accomplish some task.
For instance, a robot that wants to grab a cup from the kitchen table first needs to detect where the cup is, perhaps by moving around the table and capturing images as it goes.
Here, the robot is engaging in active perception towards the end goal of grasping the cup.&lt;/p&gt;

&lt;p&gt;To put it another way, &lt;a class=&quot;citation&quot; href=&quot;#Bajcsy_Revisiting&quot;&gt;(Bajcsy et al., 2018)&lt;/a&gt; states that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;[a]n agent is an active perceiver if it knows why it wishes to sense, and then chooses what to perceive, and determines how, when and where to achieve that perception.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In multi-agent active perception, a team of agents collaboratively engages in active perception.
You can think of a group of search and rescue robots attempting to locate victims of a disaster, or a team of exploration rovers collecting scientific samples on the surface of a planet.
My recent research concerns the theory of such active perception problems viewed as sequential decision-making tasks.
Each agent takes an individual action, and then perceives an individual observation.
The agents then select the next action, and the process repeats until termination.
We considered a very general case, with the following properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The agents act independently, and there is no implicit communication or information sharing between agents&lt;/li&gt;
  &lt;li&gt;The state undergoes a stochastic state transitions conditional on the individual actions of all agents&lt;/li&gt;
  &lt;li&gt;The agents do not know the underlying true state of the system, but rather perceive individual observations correlated with the state.&lt;/li&gt;
  &lt;li&gt;The objective of the agents is to reduce uncertainty about the hidden state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Such problems are appropriately modelled as decentralized partially observable Markov decision processes, or Dec-POMDPs.
To give a bit of context for our work, I will first review some formalizations of sequential decision-making that underlie planning and reinforcement learning.
These are the Markov decision process (MDP), the partially observable MDP (POMDP), and finally the multiagent generalization, the decentralized POMDP (Dec-POMDP).
Finally, I will discuss multi-agent active perception modelled as a Dec-POMDP, which the two papers referenced above are all about.&lt;/p&gt;

&lt;h2 id=&quot;planning-and-reinforcement-learning-formalisms&quot;&gt;Planning and reinforcement learning formalisms&lt;/h2&gt;
&lt;p&gt;Markov decision processes formalize decision-making tasks with different kinds of underlying assumptions.
Planning and reinforcement learning methods may be applied to solve the decision-making problem.&lt;/p&gt;

&lt;h3 id=&quot;full-state-observability---markov-decision-process-mdp&quot;&gt;Full state observability - Markov decision process (MDP)&lt;/h3&gt;
&lt;p&gt;The basic concepts found in all Markov decision processes are &lt;em&gt;state&lt;/em&gt;, &lt;em&gt;action&lt;/em&gt;, and &lt;em&gt;reward&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The state of the system in a Markov decision process is a collection of all features and variables relevant to the problem at hand.
For example, the state in a navigation problem might be the position of the agent in the environment.&lt;/li&gt;
  &lt;li&gt;The actions are the choices available to the agent.
For instance, the navigating agent from above might choose to move east, west, north, or south.
Each action typically has some effect on the state: moving east will change the state of the agent so it is more to the east, and so on.&lt;/li&gt;
  &lt;li&gt;Finally, rewards encode the task of the agent.
The reward depends on the current state and the current action.
The greater the reward, the more desirable it is to take the action in the state.
The navigating agent’s goal can be described through the reward: it can obtain a large positive reward when it reaches the goal location.
On the other hand, the reward can model undesirable states: moving off a cliff ledge will result in a large negative reward.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interaction of the agent and the system in a MDP proceeds as follows.
The system is in some state \(s\) which is known to the agent.
The agent then selects an action \(a\) to execute.
The agent will obtain a reward \(r \), and the system state will transition to a new state \(s'\).
Then the process is repeated in the new state.&lt;/p&gt;

&lt;p&gt;The state transitions in a MDP are not deterministic, that is, they cannot always be precisely predicted.
As a concrete example, if the agent moves east, it might end in a location exactly to the east with some &lt;em&gt;probability&lt;/em&gt;, and with some probability, it will end up in a location that is slightly off from the expected location.
The state transitions are assumed to be governed by a transition model \( T(s', a, s)\) that gives the conditional probability of the next state \(s'\) given the current state \(s\) and action \( a\).
It is also assumed that the rewards are governed by a reward model \(R(s, a)\) that gives the immediate reward of executing action \(a\) in state \(s\).&lt;/p&gt;

&lt;p&gt;The key question in planning and reinforcement learning is how the agent should act in a given state to maximize the expected sum of rewards it can collect over a long time horizon.
Rewards that are obtained quickly are considered more valuable than those far in the future.
This is modelled by a &lt;em&gt;discount factor&lt;/em&gt; \( \gamma\), a number between 0 and 1.
Rewards \(k\) steps in the future are discounted by \(\gamma^k\).
The question of how to act is answered by a &lt;em&gt;policy&lt;/em&gt; that maps states to actions.
We denote the policy as a function \(\pi \), and the action selected by the policy in state \(s\) by \(\pi(s) = a\).&lt;/p&gt;

&lt;p&gt;How good a policy is is measured by its value.
Particularly useful is the concept of an &lt;em&gt;action-value function&lt;/em&gt; \(Q^\pi \) of a policy \(\pi\).
The action-value function takes as input a state \(s\) and an action \( a\), and returns the expected sum of rewards obtained when first executing action \(a\) in the starting state \(s\), and then acting according to the policy \(\pi\) from there on.
The action value function is defined as
\begin{equation}
Q^{\pi}(s,a) \triangleq \mathbb{E}\left[ R(s_0, a_0) + \sum\limits_{t=1}^{\infty}\gamma^t R(s_t, \pi(s_t)) \middle| s_0 = s, a_0 = a\right].
\end{equation}
We can also define the &lt;em&gt;value function&lt;/em&gt; of \(\pi\) in a state \( s\) as
\begin{equation}
V^{\pi}(s) \triangleq \mathbb{E}\left[ \sum\limits_{t=0}^{\infty}\gamma^t R(s_t, \pi(s_t)) \middle| s_0 = s\right].
\end{equation}
An optimal policy maximizes the value of every state, that is, a policy \(\pi^*\) is optimal if and only if for every policy \( \pi\) and every state \(s\), \(V^{\pi^*}(s) \geq V^{\pi}(s)\).&lt;/p&gt;

&lt;p&gt;In a reinforcement learning (RL) setting, The transition model \(T\) and the reward model \(R\) are not known to the agent.
Instead, the agent interacts with the environment, recording a sequence of states, actions, and rewards.
In model-free reinforcement learning, the objective is to directly learn the action-value function \(Q^{\pi^*}\) of an optimal policy.
In model-based RL, estimate models \(\hat{T}\) and \( \hat{R}\) are learned, and then used to compute an optimal policy.
The estimated models are used as a simulator to compute an optimal policy offline, for which no interaction is required.
The process of computing an optimal policy from given or estimated models is known as &lt;em&gt;planning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A famous example of model-free RL is the deep Q network &lt;a class=&quot;citation&quot; href=&quot;#mnih2013playing&quot;&gt;(Mnih et al., 2013)&lt;/a&gt; that learns to play Atari games.
In this case, the state \(s \) is the last few video frames of the game, and the action-value function \(Q^{\pi}(s,a)\) determines the value of every game action \(a\), such as pushing a button or moving the joystick.
The best action to select is determined according to the action-value function as \( a_t = \pi(s_t) = \operatorname{arg max}\limits_{a_t} Q^{\pi}(s_t, a_t)\).&lt;/p&gt;

&lt;h3 id=&quot;partial-state-observability---partially-observable-mdp-pomdp&quot;&gt;Partial state observability - partially observable MDP (POMDP)&lt;/h3&gt;
&lt;p&gt;A limitation of the MDP formalization is that the state is assumed to be known to the agent.
This is not always the case.
For example, in some of the Atari games mentioned above, a few video frames may not be enough to determine the velocity of a moving object.
Arguably the velocity of an object is part of the state, so this means that the game-playing agent cannot actually perceive the state from the given input!&lt;/p&gt;

&lt;p&gt;In such cases, the state is &lt;em&gt;partially observable&lt;/em&gt;.
The MDP framework can be changed as follows to account for this.
First, the state will be “hidden” from the agent – it can no longer know what the state \( s\) of the system is.
Secondly, &lt;em&gt;observations&lt;/em&gt; are added to the framework.
Observations are correlated with the true underlying state of the system.
This is modelled via an &lt;em&gt;observation model&lt;/em&gt; \(O(z', s', a)\) that gives the conditional probability of perceiving observation \( z'\) given the state \(s'\) and the previous action \( a\).
As before, in an RL setting, \(O \) along with the transition model \( T\) and reward model \( R\) are not known to the agent.
These two changes give the partially observable MDP (POMDP).
There is a price to pay for the added expressiveness of the POMDP model, however, as we shall see next.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/assets/images/research/pomdp.png&quot;&gt;&lt;img src=&quot;/assets/images/research/pomdp.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Decision-making process in a POMDP&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As the state is no longer observable, the experiences of the agent are now sequences of actions, rewards and observations.
That is, an experience of length-\(t \) is a sequence \(h_t = (a_0, r_0, z_1, a_1, r_1, z_2, \ldots, a_{t-1}, z_t ) \).
The sequence \(h_t\) is also called a &lt;em&gt;history&lt;/em&gt; of actions and observations.&lt;/p&gt;

&lt;p&gt;The policy \(\pi\) also has to be changed: since the agent does not know \( s\), a policy of the form \(\pi(s) = a\) is no longer viable.
Instead, the policy should map histories to the next action: \( \pi(h_t) = a_t \).
This presents a problem.
As time goes on, the size of the history \(h_t \) increases.
Soon, it will become infeasible to store the history, let alone represent a policy function that maps any possible history to an action!&lt;/p&gt;

&lt;p&gt;The solution to the problem is to summarize the history \(h_t \) into some fixed-size representation \( b_t\).&lt;/p&gt;

&lt;p&gt;In model-free RL, this is nowadays often done by applying a recurrent neural network such as a long-short-term memory (LSTM) unit.
The internal state of the LSTM is used as the summary \(b_t\).
The LSTM \(f\) is then used to update the internal representation given a new action \(a_t\) and a new  observation \(z_{t+1}\): \(b_{t+1} = f(b_t, a_t, z_{t+1})\).
In model-based RL and planning, the summary \(b_t\) is defined as the &lt;em&gt;belief state&lt;/em&gt;, a probability over the state \(s_t\) given the history \(h_t\): \(b_t(s_t) \triangleq \mathbb{P}(s_t \mid a_0, z_1, \ldots, a_{t-1}, z_t)\).
The estimated models \( \hat{T}\) and \( \hat{O}\) are applied to compute the updated \(b_{t+1}\) by applying Bayesian filtering.
Given the previous belief state \(b_t\) and a new action \(a_t\) and a new  observation \(z_{t+1}\), the updated belief for state \(s_{t+1}\) is
\begin{equation}
b_{t+1}(s_{t+1}) \triangleq \eta \hat{O}(z_{t+1}, s_{t+1}, a_t) \sum\limits_{s_t}\hat{T}(s_{t+1},a_t,s_t)b_t(s_t),
\end{equation}
where \(\eta\) is a normalization factor.&lt;/p&gt;

&lt;p&gt;Since \(b_t\) is now a fixed-size summary of the history, it is possible to represent the action-value function \(Q^{\pi}(b_t, a_t)\), and apply it much in the same way as in the MDP case to select actions.
A good example of this is the deep recurrent Q network (DRQN) &lt;a class=&quot;citation&quot; href=&quot;#hausknecht2015deep&quot;&gt;(Hausknecht &amp;amp; Stone, 2015)&lt;/a&gt;, that was again applied to play Atari games.
We also applied the DRQN to the problem of active visual object search in &lt;a class=&quot;citation&quot; href=&quot;#Schmid_IROS2019&quot;&gt;(Schmid et al., 2019)&lt;/a&gt;.
The hidden state is the location of the target object, while the agent’s observations are the images it can see from its current location.
The actions of the agent are the different movement actions, and some special actions that declare the object to be found (active termination).
Positive rewards are accrued for successfully finding the target object, while negative rewards are accumulated for false declarations and movements.
&lt;!-- [Here's a video](assets/videos/schmid-lauri-frintrop-iros-2019-video.mp4){:target=&quot;_blank&quot;} that shows how the object search task proceeds. --&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;a href=&quot;/assets/images/research/subtasks.png&quot;&gt;&lt;img src=&quot;/assets/images/research/subtasks.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Active visual object search can be formalized as a POMDP.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;multiple-agents---decentralized-pomdp-dec-pomdp&quot;&gt;Multiple agents - decentralized POMDP (Dec-POMDP)&lt;/h3&gt;
&lt;p&gt;Now we have gone from the MDP to the POMDP by hiding the state from the agent, and by adding observations.
The decentralized POMDP (Dec-POMDP) is a generalization of a POMDP with multiple cooperating agents.
For simplicity, I will consider here the case with two agents, but the Dec-POMDP is applicable to any finite number \(n\) of agents.
For an in-depth introduction to Dec-POMDPs, I recommend the book by Oliehoek and Amato &lt;a class=&quot;citation&quot; href=&quot;#Oliehoek2016&quot;&gt;(Oliehoek &amp;amp; Amato, 2016)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Both agents are independent in the sense that they take their own &lt;em&gt;individual&lt;/em&gt; actions, and perceive their own &lt;em&gt;individual&lt;/em&gt; observations.
Therefore, they experience an &lt;em&gt;individual&lt;/em&gt; history, and have &lt;em&gt;individual&lt;/em&gt; policies.
The system state transitions, observations, and rewards however still depend on the &lt;em&gt;joint&lt;/em&gt; actions of the agents.
A joint action is a tuple \(a_t = \langle a_{1,t}, a_{2,t}\rangle\) of both agents’ individual actions \(a_{i,t}\).
The transition model is defined on these joint actions, that is,
\begin{equation}
T(s_{t+1}, a_t, s_t) = T(s_{t+1}, a_{1,t}, a_{2,t}, s_t)
\end{equation}
gives the conditional probability of the next state being \(s_{t+1}\) when the agent 1 takes an individual action \( a_{1,t}\), and agent 2 takes an individual action \( a_{2,t}\), and the previous state is \( s_t\).
Similarly, a joint observation is a tuple \( z_t = \langle z_{1,t}, z_{2,t} \rangle\) of individual observations \(z_{i,t}\).
The observation model defines the conditional probability of a joint observation given the state and the previous joint action:
\begin{equation}
O(z_{t+1}, s_{t+1}, a_t) = O(z_{1,t+1}, z_{2,t+1}, s_{t+1}, a_{1,t}, a_{2,t}).
\end{equation}
The rewards in a Dec-POMDP are shared.
The reward model is defined
\begin{equation}
R(s_t, a_t) = R(s_t, a_{1,t}, a_{2,t}),
\end{equation}
determining the reward of taking joint action \(a_t = \langle a_{1,t}, a_{2,t}\rangle\) in state \(s_t\).&lt;/p&gt;

&lt;p&gt;One agent does not know precisely how the other agent acted, or what the other agent perceived.
Each agent has its own individual history \(h_{i,t} = (a_{i,0}, z_{i,1}, \ldots, a_{i,t-1}, z_{i,t})\).
Therefore, the individual policy of each agent maps individual histories to actions: \( \pi_{i}(h_{i,t}) = a_{i,t}\).
A &lt;em&gt;joint policy&lt;/em&gt; is a tuple of individual policies, that is \( \pi = \langle \pi_1, \pi_2 \rangle\).
Because information is not shared, it is not possible anymore to come up with a single global summary \(b_t\) of the hidden state as in the POMDP.
Instead, each agent can either remember its individual history, or create an individual summary \( b_{i,t}\) of it.&lt;/p&gt;

&lt;p&gt;The paradigm of centralized planning, decentralized execution is assumed in Dec-POMDPs.
Individual policies of each agent are designed in a centralized manner, but they can be executed by each agent independently, without knowing what the other agents are doing.
This means that we can reason about the &lt;em&gt;joint summary&lt;/em&gt; \(b_t\) of the agents during planning, even if no individual agent has the capability to compute this summary at execution-time.
This reasoning is the key part of multi-agent active perception modelled as a Dec-POMDP.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/assets/images/research/decpomdp.png&quot;&gt;&lt;img src=&quot;/assets/images/research/decpomdp.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Decision-making process in a Dec-POMDP&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;multi-agent-active-perception&quot;&gt;Multi-agent active perception&lt;/h2&gt;
&lt;p&gt;In some active perception problems there is a clear state-based objective.
These types of problems can be thought of as active hypothesis testing.
For example in the active visual object search example discussed earlier, there is a set of competing hypotheses (possible locations of the target object), and the agent is trying to discriminate between these objectives.
The agent is rewarded for successfully declaring the correct hypothesis, and penalized for mistakes or taking too much time.&lt;/p&gt;

&lt;p&gt;However, some active perception problems are more open-ended, or curiosity driven.
Consider for example a monitoring task, like the task of monitoring ocean currents.
There is no state-based objective or desirable state to reach – the currents are what they are and change over time – so there is no right hypothesis to call.
Instead, the objective would be to reduce uncertainty about the hidden state in the problem.
These types of active perception problems are exactly the type we consider in a multi-agent setting in &lt;a class=&quot;citation&quot; href=&quot;#Lauri_JAAMAS2020&quot;&gt;(Lauri et al., 2020; Lauri et al., 2019)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We modify the reward function of the problem, so that it is not dependent on the state, but rather on &lt;em&gt;what the agents know about the state&lt;/em&gt;, that is, the summary or belief state \(b_t\) discussed earlier.
Uncertainty of a belief state can be quantified by using the information entropy.
Recall the belief state \(b_t\) defined as a probability over possible hidden state \(s_t\).
The entropy of the belief state is
\begin{equation}
H(b_t) \triangleq -\sum\limits_{s_t}b_t(s_t) \ln b_t(s_t).
\end{equation}
Applying the negative entropy as a reward function makes the agents prefer policies that are likely to lead to a belief state with low uncertainty.&lt;/p&gt;

&lt;p&gt;We were able to derive some useful theoretical results for these types of active perception problems in the multi-agent setting.
We established that the action-value function \(Q^{\pi}(b_t,a_t)\) and the value function \(V^{\pi}(b_t)\) of any joint policy \( \pi \) are convex functions of the belief state \(b_t\).
This lead to a practical algorithm for planning policies for active perception.&lt;/p&gt;

&lt;p&gt;For our algorithm, we chose to represent individual policies as finite state controllers, or policy graphs, as shown in the figure below.
Each agent starts executing a fixed-size policy graph from an initial controller node shown on the left hand side of the respective figures.
Then, the agent takes the action read from the table for the controller node.
The individual observation perceived by the agent then determines what is the next controller node it transitions to: this is modelled by the edges of the policy graph.
This approach produces compact and easily understandable policies, while also limiting the search space.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/assets/images/active-perception/localpolicy_agent1.png&quot;&gt;&lt;img src=&quot;/assets/images/active-perception/localpolicy_agent1.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/assets/images/active-perception/localpolicy_agent2.png&quot;&gt;&lt;img src=&quot;/assets/images/active-perception/localpolicy_agent2.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Left: Individual policy of agent 1. Right: Individual policy of agent 2.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The core idea of our planning algorithm is to iteratively improve the policy graphs.
Improvements are done by 1) changing the individual action to be taken at a controller node, and 2) changing the out-edge configuration of a controller node.
If we are improving a node, say, \(q_1^1\), we answer questions such as: “if agent 1 is in controller node \(q_1^1 \), what are the probabilities of the possible controller nodes where agent 2 is?”.
Then, we also consider the belief states in a similar manner: “if agent 1 is in controller node \(q_1^1\) and agent 2 is in controller node \(q_2^2 \), what are the possible belief states and what are their relative likelihoods?”.
Using this information, we are able to calculate if changing the individual action or where out-edges are connected to at node \(q_1^1\) would result in a greater expected value for the joint policy.&lt;/p&gt;

&lt;p&gt;We applied the algorithm to two multi-agent active perception problems.
In one of the problems, two micro air vehicles (MAVs) are tracking a moving target.
The target is either friendly or hostile.
A hostile target will tend to move quickly in order to avoid detection, while a friendly target tends to move more slowly.
The two MAV agents can either use a camera or radar to detect the target.
The camera is reliable on short ranges, but does not work well with a longer range.
The radar in turn works well with long ranges to the target, but if both MAVs use their radar simultaneously the accuracy suffers due to interference.
The camera and radar measurements only provide information about the location of the target.
The type (friendly or hostile) of the target must be inferred from a longer sequence of observations, tracking the behaviour of the target.
In earlier work &lt;a class=&quot;citation&quot; href=&quot;#Lauri_ICRA2017&quot;&gt;(Lauri et al., 2017)&lt;/a&gt;, we applied a similar model to a real-world problem of two robots tracking a target, as shown in the image below.&lt;/p&gt;
&lt;figure&gt;
    &lt;a href=&quot;/assets/images/active-perception/experiment.jpg&quot;&gt;&lt;img src=&quot;/assets/images/active-perception/experiment.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Two robots tracking a moving target.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Using our proposed algorithm, we were able to find efficient and compact individual tracking policies .
By using particle filtering to track the belief state, we could extend the algorithm to the case where the hidden state \(s_t\) is continuous, while the actions and observations are discrete.
We demonstrated this variant of the algorithm in a signal source localization task in &lt;a class=&quot;citation&quot; href=&quot;#Lauri_JAAMAS2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are interested in trying out this algorithm for yourself, either to solve multi-agent active perception problems or regular Dec-POMDPs, you can find the code &lt;a href=&quot;https://github.com/laurimi/npgi&quot;&gt;on Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The limitations of our algorithm are discussed in &lt;a class=&quot;citation&quot; href=&quot;#Lauri_JAAMAS2020&quot;&gt;(Lauri et al., 2020)&lt;/a&gt;.
Firstly, at each improvement round we need to compute all belief states reachable under the current policy.
This requires a lot of memory, which will limit the scalability of the algorithm.
Secondly, the size of the policy graph has to be fixed beforehand.
Choosing a too small policy graph might prevent finding good policies, while using a too large policy graph will waste computational resources.
Finally, because the reward function depends on the belief state and not the state, we have to use a Bayes filter to compute belief states at planning time.
As a result, the scale of the problems that can currently be solved is not quite up to par with state-of-the-art solvers for Dec-POMDPs with state-based rewards.
The algorithm we proposed is heuristic – although it performed well experimentally, it does not provide any performance guarantees compared to an optimal solution.
These are among some of the issues we hope to address in future work.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This post gives a brief overview of different variants of Markov decision processes and how they are used as underlying formalisms for reinforcement learning and planning.
In active perception problems, an agent or a team of agents perceive the world to achieve some goal state or simply to reduce uncertainty in their knowledge of the state.
Using a decentralized partially observable Markov decision process (Dec-POMDP) as an underlying framework, it is possible to define very general multi-agent active perception problems.
No implicit information sharing is assumed, the underlying state is only partially observable, while the system undergoes stochastic state transitions.&lt;/p&gt;

&lt;p&gt;Our recent research provides some theoretical grounding for multi-agent active perception as a Dec-POMDP, and proposes the first heuristic algorithm for solving such problems.
As discussed above, the approach is not without limitations.
I believe there are many further properties of Dec-POMDPs for multi-agent active perception to be discovered.
These will hopefully lead to even more powerful solution algorithms as well.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Lauri_JAAMAS2020&quot;&gt;&lt;b&gt;Lauri, M.&lt;/b&gt;, Pajarinen, J., &amp;amp; Peters, J. (2020). Multi-agent active information gathering in discrete and continuous-state decentralized POMDPs by policy graph improvement. &lt;i&gt;Autonomous Agents and Multi-Agent Systems&lt;/i&gt;, &lt;i&gt;34&lt;/i&gt;(42). https://doi.org/10.1007/S10458-020-09467-6&lt;/span&gt;&lt;/div&gt;



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1007/S10458-020-09467-6&quot;&gt;DOI&lt;/a&gt;]



	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/laurimi/npgi&quot;&gt;Code&lt;/a&gt;]



[&lt;a onclick=&quot;toggleBibtexLauri_JAAMAS2020()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexLauri_JAAMAS2020()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aLauri_JAAMAS2020&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bLauri_JAAMAS2020&quot;&gt;@article{Lauri_JAAMAS2020,
  author = {Lauri, Mikko and Pajarinen, Joni and Peters, Jan},
  title = {Multi-agent active information gathering in discrete and continuous-state decentralized POMDPs by policy graph improvement},
  year = {2020},
  volume = {34},
  number = {42},
  doi = {10.1007/S10458-020-09467-6},
  journal = {Autonomous Agents and Multi-Agent Systems},
  codelink = {https://github.com/laurimi/npgi}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexLauri_JAAMAS2020(parameter) {
    var x= document.getElementById('aLauri_JAAMAS2020');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexLauri_JAAMAS2020(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bLauri_JAAMAS2020').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Lauri_AAMAS2019&quot;&gt;&lt;b&gt;Lauri, M.&lt;/b&gt;, Pajarinen, J., &amp;amp; Peters, J. (2019). Information Gathering in Decentralized POMDPs by Policy Graph Improvement. &lt;i&gt;Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)&lt;/i&gt;, 1143–1151. https://dl.acm.org/doi/abs/10.5555/3306127.3331815&lt;/span&gt;&lt;/div&gt;


	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://dl.acm.org/doi/abs/10.5555/3306127.3331815&quot;&gt;URL&lt;/a&gt;]



	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://arxiv.org/abs/1902.09840&quot;&gt;arXiv&lt;/a&gt;]


	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/laurimi/npgi&quot;&gt;Code&lt;/a&gt;]



[&lt;a onclick=&quot;toggleBibtexLauri_AAMAS2019()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexLauri_AAMAS2019()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aLauri_AAMAS2019&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bLauri_AAMAS2019&quot;&gt;@inproceedings{Lauri_AAMAS2019,
  author = {Lauri, Mikko and Pajarinen, Joni and Peters, Jan},
  title = {Information Gathering in Decentralized POMDPs by Policy Graph Improvement},
  year = {2019},
  isbn = {9781450363099},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  pages = {1143–1151},
  numpages = {9},
  keywords = {planning under uncertainty, decentralized pomdps, information theory, multi-agent planning},
  location = {Montreal QC, Canada},
  series = {AAMAS ’19},
  codelink = {https://github.com/laurimi/npgi},
  archiveprefix = {arXiv},
  eprint = {1902.09840},
  url = {https://dl.acm.org/doi/abs/10.5555/3306127.3331815}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexLauri_AAMAS2019(parameter) {
    var x= document.getElementById('aLauri_AAMAS2019');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexLauri_AAMAS2019(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bLauri_AAMAS2019').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Bajcsy_Revisiting&quot;&gt;Bajcsy, R., Aloimonos, Y., &amp;amp; Tsotsos, J. K. (2018). Revisiting active perception. &lt;i&gt;Autonomous Robots&lt;/i&gt;, &lt;i&gt;42&lt;/i&gt;(2), 177–196.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexBajcsy_Revisiting()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexBajcsy_Revisiting()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aBajcsy_Revisiting&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bBajcsy_Revisiting&quot;&gt;@article{Bajcsy_Revisiting,
  title = {Revisiting active perception},
  author = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K},
  journal = {Autonomous Robots},
  volume = {42},
  number = {2},
  pages = {177--196},
  year = {2018}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexBajcsy_Revisiting(parameter) {
    var x= document.getElementById('aBajcsy_Revisiting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexBajcsy_Revisiting(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bBajcsy_Revisiting').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;mnih2013playing&quot;&gt;Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp;amp; Riedmiller, M. (2013). Playing atari with deep reinforcement learning. &lt;i&gt;ArXiv Preprint ArXiv:1312.5602&lt;/i&gt;.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexmnih2013playing()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexmnih2013playing()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;amnih2013playing&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bmnih2013playing&quot;&gt;@article{mnih2013playing,
  title = {Playing atari with deep reinforcement learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:1312.5602},
  year = {2013}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexmnih2013playing(parameter) {
    var x= document.getElementById('amnih2013playing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexmnih2013playing(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bmnih2013playing').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;hausknecht2015deep&quot;&gt;Hausknecht, M., &amp;amp; Stone, P. (2015). Deep recurrent Q-learning for partially observable MDPs. &lt;i&gt;2015 AAAI Fall Symposium Series&lt;/i&gt;.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexhausknecht2015deep()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexhausknecht2015deep()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;ahausknecht2015deep&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bhausknecht2015deep&quot;&gt;@inproceedings{hausknecht2015deep,
  title = {{Deep recurrent Q-learning for partially observable MDPs}},
  author = {Hausknecht, Matthew and Stone, Peter},
  booktitle = {2015 AAAI Fall Symposium Series},
  year = {2015}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexhausknecht2015deep(parameter) {
    var x= document.getElementById('ahausknecht2015deep');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexhausknecht2015deep(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bhausknecht2015deep').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Schmid_IROS2019&quot;&gt;Schmid, J. F., &lt;b&gt;Lauri, M.&lt;/b&gt;, &amp;amp; Frintrop, S. (2019, November). Explore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning. &lt;i&gt;IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)&lt;/i&gt;. https://doi.org/10.1109/IROS40897.2019.8967805&lt;/span&gt;&lt;/div&gt;

	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;/repository/Schmid_IROS2019.pdf&quot;&gt;PDF&lt;/a&gt;]



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1109/IROS40897.2019.8967805&quot;&gt;DOI&lt;/a&gt;]



	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/JanFabianSchmid/RL_for_AVOS&quot;&gt;Code&lt;/a&gt;]


	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://www.inf.uni-hamburg.de/en/inst/ab/cv/media/schmid-lauri-frintrop-iros-2019-video.mp4&quot;&gt;Video&lt;/a&gt;]


[&lt;a onclick=&quot;toggleBibtexSchmid_IROS2019()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexSchmid_IROS2019()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aSchmid_IROS2019&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bSchmid_IROS2019&quot;&gt;@inproceedings{Schmid_IROS2019,
  author = {Schmid, Jan Fabian and Lauri, Mikko and Frintrop, Simone},
  title = {{Explore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning}},
  booktitle = {{IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)}},
  year = {2019},
  month = nov,
  address = {Macau, China},
  doi = {10.1109/IROS40897.2019.8967805},
  codelink = {https://github.com/JanFabianSchmid/RL_for_AVOS},
  videolink = {https://www.inf.uni-hamburg.de/en/inst/ab/cv/media/schmid-lauri-frintrop-iros-2019-video.mp4}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexSchmid_IROS2019(parameter) {
    var x= document.getElementById('aSchmid_IROS2019');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexSchmid_IROS2019(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bSchmid_IROS2019').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Oliehoek2016&quot;&gt;Oliehoek, F. A., &amp;amp; Amato, C. (2016). &lt;i&gt;A Concise Introduction to Decentralized POMDPs&lt;/i&gt;. Springer.&lt;/span&gt;&lt;/div&gt;







[&lt;a onclick=&quot;toggleBibtexOliehoek2016()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexOliehoek2016()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aOliehoek2016&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bOliehoek2016&quot;&gt;@book{Oliehoek2016,
  author = {Oliehoek, Frans A and Amato, Christopher},
  title = {{A Concise Introduction to Decentralized POMDPs}},
  year = {2016},
  publisher = {Springer}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexOliehoek2016(parameter) {
    var x= document.getElementById('aOliehoek2016');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexOliehoek2016(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bOliehoek2016').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;&lt;span id=&quot;Lauri_ICRA2017&quot;&gt;&lt;b&gt;Lauri, M.&lt;/b&gt;, Heinänen, E., &amp;amp; Frintrop, S. (2017). Multi-Robot Active Information Gathering with Periodic Communication. &lt;i&gt;IEEE Intl. Conf. on Robotics and Automation (ICRA)&lt;/i&gt;, 851–856. https://doi.org/10.1109/ICRA.2017.7989104&lt;/span&gt;&lt;/div&gt;



    [&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;http://doi.org/10.1109/ICRA.2017.7989104&quot;&gt;DOI&lt;/a&gt;]


	[&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://arxiv.org/abs/1703.02610&quot;&gt;arXiv&lt;/a&gt;]




[&lt;a onclick=&quot;toggleBibtexLauri_ICRA2017()&quot;&gt;Show/hide BibTeX&lt;/a&gt;]
[&lt;a onclick=&quot;copyBibtexLauri_ICRA2017()&quot;&gt;Copy BibTeX to clipboard&lt;/a&gt;]
&lt;div id=&quot;aLauri_ICRA2017&quot; style=&quot;display: none;&quot;&gt;
&lt;pre id=&quot;bLauri_ICRA2017&quot;&gt;@inproceedings{Lauri_ICRA2017,
  author = {Lauri, Mikko and Heinänen, Eero and Frintrop, Simone},
  title = {{Multi-Robot Active Information Gathering with Periodic Communication}},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2017},
  pages = {851-856},
  month = may,
  address = {Singapore},
  doi = {10.1109/ICRA.2017.7989104},
  eprint = {1703.02610}
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;script&gt;
function toggleBibtexLauri_ICRA2017(parameter) {
    var x= document.getElementById('aLauri_ICRA2017');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}

function copyBibtexLauri_ICRA2017(parameter) {
	// https://stackoverflow.com/a/30810322/5471520
	var textArea = document.createElement(&quot;textarea&quot;);
	//
	// *** This styling is an extra step which is likely not required. ***
	//
	// Why is it here? To ensure:
	// 1. the element is able to have focus and selection.
	// 2. if element was to flash render it has minimal visual impact.
	// 3. less flakyness with selection and copying which **might** occur if
	//    the textarea element is not visible.
	//
	// The likelihood is the element won't even render, not even a
	// flash, so some of these are just precautions. However in
	// Internet Explorer the element is visible whilst the popup
	// box asking the user for permission for the web page to
	// copy to the clipboard.
	//

	// Place in top-left corner of screen regardless of scroll position.
	textArea.style.position = 'fixed';
	textArea.style.top = 0;
	textArea.style.left = 0;

	// Ensure it has a small width and height. Setting to 1px / 1em
	// doesn't work as this gives a negative w/h on some browsers.
	textArea.style.width = '2em';
	textArea.style.height = '2em';

	// We don't need padding, reducing the size if it does flash render.
	textArea.style.padding = 0;

	// Clean up any borders.
	textArea.style.border = 'none';
	textArea.style.outline = 'none';
	textArea.style.boxShadow = 'none';

	// Avoid flash of white box if rendered for any reason.
	textArea.style.background = 'transparent';

	textArea.value = document.getElementById('bLauri_ICRA2017').textContent;

	document.body.appendChild(textArea);
	textArea.focus();
	textArea.select();

	try {
	  var successful = document.execCommand('copy');
	  var msg = successful ? 'successful' : 'unsuccessful';
	  console.log('Copying text command was ' + msg);
	} catch (err) {
	  console.log('Oops, unable to copy');
	}

	document.body.removeChild(textArea);
} 
&lt;/script&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Mikko Lauri</name></author><summary type="html">In this post, I give an overview of my recent research on multi-agent active perception, published in the two recent papers (Lauri et al., 2020; Lauri et al., 2019).</summary></entry><entry><title type="html">Website started</title><link href="https://laurimi.github.io/news/2020/05/10/welcome.html" rel="alternate" type="text/html" title="Website started" /><published>2020-05-10T00:00:00+02:00</published><updated>2020-05-10T00:00:00+02:00</updated><id>https://laurimi.github.io/news/2020/05/10/welcome</id><content type="html" xml:base="https://laurimi.github.io/news/2020/05/10/welcome.html">&lt;p&gt;Today, the first version of the website is up.
I used &lt;a href=&quot;https://jekyllrb.com&quot; target=&quot;_blank&quot;&gt;jekyll&lt;/a&gt;, along with the &lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot; target=&quot;_blank&quot;&gt;minimal-mistakes&lt;/a&gt; theme and &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot; target=&quot;_blank&quot;&gt;jekyll-scholar&lt;/a&gt; plugin for citations.&lt;/p&gt;</content><author><name>Mikko Lauri</name></author><summary type="html">Today, the first version of the website is up. I used jekyll, along with the minimal-mistakes theme and jekyll-scholar plugin for citations.</summary></entry></feed>